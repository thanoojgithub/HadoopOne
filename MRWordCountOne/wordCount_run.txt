thanooj@ubuntu:/$ clear

thanooj@ubuntu:/$ hdfs namenode -format
.....
Re-format filesystem in Storage Directory /home/thanooj/hadoop-2.7.3/hdfs/namenode ? (Y or N) Y
.....
16/11/12 10:46:25 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
thanooj@ubuntu:/$ 
thanooj@ubuntu:~$ start-all.sh
thanooj@ubuntu:~$ jps
5169 NodeManager
4513 DataNode
5303 Jps
4857 ResourceManager
4362 NameNode
4703 SecondaryNameNode
thanooj@ubuntu:~$


thanooj@ubuntu:~$ hdfs dfs -ls /
thanooj@ubuntu:~$ hdfs dfs -mkdir /input
thanooj@ubuntu:~$ hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - thanooj supergroup          0 2016-11-12 13:29 /input
thanooj@ubuntu:~$ hdfs dfs -mkdir /output
thanooj@ubuntu:~$ hdfs dfs -ls /
Found 2 items
drwxr-xr-x   - thanooj supergroup          0 2016-11-12 13:29 /input
drwxr-xr-x   - thanooj supergroup          0 2016-11-12 13:29 /output
thanooj@ubuntu:~$ 


thanooj@ubuntu:~/input$ cat word
i am a good boy
i am a bad boy
you are a good boy
you are a bad boy
thanooj@ubuntu:~/input$ 

thanooj@ubuntu:~/input$ hdfs dfs -put /home/thanooj/input/word /input

thanooj@ubuntu:~$ hdfs dfs -ls /
Found 2 items
drwxr-xr-x   - thanooj supergroup          0 2016-11-12 13:37 /input
drwxr-xr-x   - thanooj supergroup          0 2016-11-12 13:29 /output
thanooj@ubuntu:~$ hdfs dfs -ls /input
Found 1 items
-rw-r--r--   1 thanooj supergroup         68 2016-11-12 13:37 /input/word
thanooj@ubuntu:~$ hdfs dfs -cat /input/word
i am a good boy
i am a bad boy
you are a good boy
you are a bad boy

thanooj@ubuntu:~$ hadoop jar /home/thanooj/work/MRWordCountOne-1.0.jar com.mrwordcountone.WordCount /input/word /output/word/
16/11/12 13:53:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/11/12 13:53:32 INFO input.FileInputFormat: Total input paths to process : 1
16/11/12 13:53:32 INFO mapreduce.JobSubmitter: number of splits:1
16/11/12 13:53:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1478985990191_0002
16/11/12 13:53:32 INFO impl.YarnClientImpl: Submitted application application_1478985990191_0002
16/11/12 13:53:33 INFO mapreduce.Job: The url to track the job: http://ubuntu:8088/proxy/application_1478985990191_0002/
16/11/12 13:53:33 INFO mapreduce.Job: Running job: job_1478985990191_0002
16/11/12 13:53:42 INFO mapreduce.Job: Job job_1478985990191_0002 running in uber mode : false
16/11/12 13:53:42 INFO mapreduce.Job:  map 0% reduce 0%
16/11/12 13:53:50 INFO mapreduce.Job:  map 100% reduce 0%
16/11/12 13:54:08 INFO mapreduce.Job:  map 100% reduce 100%
16/11/12 13:54:08 INFO mapreduce.Job: Job job_1478985990191_0002 completed successfully
16/11/12 13:54:08 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=82
		FILE: Number of bytes written=238793
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=165
		HDFS: Number of bytes written=44
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5304
		Total time spent by all reduces in occupied slots (ms)=13446
		Total time spent by all map tasks (ms)=5304
		Total time spent by all reduce tasks (ms)=13446
		Total vcore-milliseconds taken by all map tasks=5304
		Total vcore-milliseconds taken by all reduce tasks=13446
		Total megabyte-milliseconds taken by all map tasks=5431296
		Total megabyte-milliseconds taken by all reduce tasks=13768704
	Map-Reduce Framework
		Map input records=4
		Map output records=20
		Map output bytes=148
		Map output materialized bytes=82
		Input split bytes=97
		Combine input records=20
		Combine output records=8
		Reduce input groups=8
		Reduce shuffle bytes=82
		Reduce input records=8
		Reduce output records=8
		Spilled Records=16
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=192
		CPU time spent (ms)=2270
		Physical memory (bytes) snapshot=307232768
		Virtual memory (bytes) snapshot=3795103744
		Total committed heap usage (bytes)=153751552
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=68
	File Output Format Counters 
		Bytes Written=44
thanooj@ubuntu:~$ hdfs dfs -ls /output/
Found 1 items
drwxr-xr-x   - thanooj supergroup          0 2016-11-12 13:54 /output/word
thanooj@ubuntu:~$ hdfs dfs -ls /output/word
Found 2 items
-rw-r--r--   1 thanooj supergroup          0 2016-11-12 13:54 /output/word/_SUCCESS
-rw-r--r--   1 thanooj supergroup         44 2016-11-12 13:54 /output/word/part-r-00000
thanooj@ubuntu:~$ hdfs dfs -cat /output/word/p*
a	4
am	2
are	2
bad	2
boy	4
good	2
i	2
you	2
thanooj@ubuntu:~$






Note:
A Combiner, also known as a semi-reducer, is an optional class that operates by accepting the inputs from the Map class and thereafter passing the output key-value pairs to the Reducer class.
The main function of a Combiner is to summarize the map output records with the same key. The output (key-value collection) of the combiner will be sent over the network to the actual Reducer task as input.

Combiner
The Combiner class is used in between the Map class and the Reduce class to reduce the volume of data transfer between Map and Reduce. Usually, the output of the map task is large and the data transferred to the reduce task is high.

How Combiner Works?
Here is a brief summary on how MapReduce Combiner works −

A combiner does not have a predefined interface and it must implement the Reducer interface’s reduce() method.
A combiner operates on each map output key. It must have the same output key-value types as the Reducer class.
A combiner can produce summary information from a large dataset because it replaces the original Map output.
As combiner can run muliple times on top of Map's output and out of the combiners are random,
we can apply combiner only when, nature of the problem valid for commutation and association 
a+b=b+a     and a+(b+c) = (a+b)+c


